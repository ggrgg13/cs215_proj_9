{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The site I'm pulling data from:https://www.planecrashinfo.com/database.htm\n",
    "For some reason it does not have a data table I can readily download, so I made a challenge for myself to extract the data from the site itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through inspecting the source from https://www.planecrashinfo.com/1920/1920.htm, I was able to determine the data exists in html tables, meaning I can extract it with relative ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.planecrashinfo.com/1920/1920.htm'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"preview.html\", \"w\", encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried printing the output directly, but it seems vscode/jupyter can't display it properly. So I wrote the output to preview.html. It produced the exact table I wanted. Next step is to extract the data by filtering for html tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the tables in the site\n",
    "table = soup.find('table')\n",
    "\n",
    "# Open a new CSV file\n",
    "with open('testOutput.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write header row manually\n",
    "    headers = [\"Date\", \"Location/Operator\", \"Aircraft_Type/Registration\", \"Fatalities\"]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    # Extract data from each row in the table and write to the CSV file\n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        columns = row.find_all('td')\n",
    "        data = [col.text.strip() for col in columns]\n",
    "        writer.writerow(data)  # Write data to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon closer inspection, the location / operator tab, as well as aircraft type / registration tab are not properly seperated, further modification need to be made to seperate them.\n",
    "current table:\n",
    "\n",
    "Date,Location/Operator,Aircraft_Type/Registration,Fatalities\n",
    "08 Jan 1935,\"Near Karachi, PakistanIndian Air Force\",Wapiti?,0/2(15)\n",
    "Expected result:\n",
    "\n",
    "Date,Location,Operator,Aircraft_Type,Registration,Fatalities\n",
    "08 Jan 1935,\"Near Karachi, Pakistan\",\"Indian Air Force\",Wapiti?,0/2(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.planecrashinfo.com/1920/1920.htm'\n",
    "response = requests.get(url)\n",
    "soup = bs(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "with open('testOutputCleaned.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Seperated headers\n",
    "    headers = [\"Date\", \"Location\", \"Operator\", \"Aircraft_Type\", \"Registration\", \"Fatalities\"]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        columns = row.find_all('td')\n",
    "        data = []\n",
    "\n",
    "        # Date\n",
    "        data.append(columns[0].get_text(strip=True))\n",
    "\n",
    "        # Location and Operator\n",
    "        loc_op = columns[1].get_text(\"|\", strip=True).split(\"|\")\n",
    "        if len(loc_op) == 2:\n",
    "            # If there are two elements after splitting, it's likely location and operator\n",
    "            data.extend(loc_op)\n",
    "        else:\n",
    "            # Handle cases where the split doesn't work as expected\n",
    "            data.extend([columns[1].get_text(strip=True), \"\"])\n",
    "\n",
    "        # Aircraft Type and Registration\n",
    "        type_reg = columns[2].get_text(\"|\", strip=True).split(\"|\")\n",
    "        if len(type_reg) == 2:\n",
    "            data.extend(type_reg)\n",
    "        else:\n",
    "            # Handle cases where the split doesn't work as expected\n",
    "            data.extend([columns[2].get_text(strip=True), \"\"])\n",
    "\n",
    "        # Fatalities\n",
    "        data.append(columns[3].get_text(strip=True))\n",
    "\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing was successful. Now to adapt it for all the tables, from 1920 to 2023. A for loop should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write header row\n",
    "    headers = [\"Year\", \"Date\", \"Location\", \"Operator\", \"Aircraft_Type\", \"Registration\", \"Fatalities\"]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    for year in range(1920, 2024):\n",
    "        time.sleep(1)  # Delay to prevent server overload\n",
    "        url = f'https://www.planecrashinfo.com/{year}/{year}.htm'\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "\n",
    "        table = soup.find('table')\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "            columns = row.find_all('td')\n",
    "            data = [year]  # Start with the year\n",
    "\n",
    "            # Date\n",
    "            data.append(columns[0].get_text(strip=True))\n",
    "\n",
    "            # Location and Operator\n",
    "            loc_op = columns[1].get_text(\"|\", strip=True).split(\"|\")\n",
    "            if len(loc_op) == 2:\n",
    "                data.extend(loc_op)\n",
    "            else:\n",
    "                data.extend([columns[1].get_text(strip=True), \"\"])\n",
    "\n",
    "            # Aircraft Type and Registration\n",
    "            type_reg = columns[2].get_text(\"|\", strip=True).split(\"|\")\n",
    "            if len(type_reg) == 2:\n",
    "                data.extend(type_reg)\n",
    "            else:\n",
    "                data.extend([columns[2].get_text(strip=True), \"\"])\n",
    "\n",
    "            # Fatalities\n",
    "            data.append(columns[3].get_text(strip=True))\n",
    "\n",
    "            writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result still produce some errors. In the original site, there are some incorrect formatting and duplicated data points, which result in the scraped data being incorrectly formatted. for example: \n",
    "1926,08 Mar 1926,\"Staaken, Germany\",Deutche Lufthansa,Junkers F-13,\"D-290\n",
    "D-290\",1/1(0)\n",
    "As such, additional cleaning need to be done to correct the data.\n",
    "In addition, changed date-time into actual date-time format, got rid of the 'year' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Remove duplicate words and replace newlines with spaces. Probably the hardest part to implement.\"\"\"\n",
    "    text = ' '.join(text.split())  # Removes newlines and extra spaces\n",
    "    words = text.split()\n",
    "    # Remove duplicates while preserving order\n",
    "    seen = set()\n",
    "    return ' '.join([x for x in words if not (x in seen or seen.add(x))])\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"Convert date string to a standard date-time format.\"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return date_str  # Return the original string if parsing fails\n",
    "\n",
    "with open('data.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write header row\n",
    "    headers = [\"Date\", \"Location\", \"Operator\", \"Aircraft_Type\", \"Registration\", \"Fatalities\"]\n",
    "    writer.writerow(headers)\n",
    "\n",
    "    for year in range(1920, 2024):\n",
    "        time.sleep(1)  # Delay to prevent server overload\n",
    "        url = f'https://www.planecrashinfo.com/{year}/{year}.htm' # parse urls in order\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'html.parser')\n",
    "\n",
    "        table = soup.find('table')\n",
    "\n",
    "        for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "            columns = row.find_all('td')\n",
    "            data = []\n",
    "\n",
    "            # Date (converted to standard format)\n",
    "            data.append(parse_date(clean_text(columns[0].get_text(strip=True))))\n",
    "\n",
    "            # Location and Operator\n",
    "            loc_op = clean_text(columns[1].get_text(\"|\", strip=True)).split(\"|\")\n",
    "            data.extend(loc_op if len(loc_op) == 2 else [columns[1].get_text(strip=True), \"\"])\n",
    "\n",
    "            # Aircraft Type and Registration\n",
    "            type_reg = clean_text(columns[2].get_text(\"|\", strip=True)).split(\"|\")\n",
    "            data.extend(type_reg if len(type_reg) == 2 else [columns[2].get_text(strip=True), \"\"])\n",
    "\n",
    "            # Fatalities\n",
    "            data.append(clean_text(columns[3].get_text(strip=True)))\n",
    "\n",
    "            writer.writerow(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
